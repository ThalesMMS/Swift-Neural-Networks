# MLP Step-by-Step Walkthrough Documentation

## Overview

This documentation provides a **detailed, line-by-line walkthrough** of how a Multi-Layer Perceptron (MLP) neural network works. It's designed for beginners who want to understand neural networks from first principles, with clear explanations, diagrams, and concrete examples.

## Purpose

Neural networks can seem like black boxes. This walkthrough opens the box and shows you exactly what happens at each step:
- How input data flows through the network
- What matrix multiplications actually do
- Why activation functions are essential
- How loss is computed
- How gradients flow backward
- How weights get updated

**No prior ML experience required.** We explain everything from scratch.

## Table of Contents

### Core Documentation
1. **[MLP Walkthrough Overview](mlp-walkthrough.md)** Start here!
   - Architecture overview
   - Training pipeline visualization
   - Code references and next steps

2. **[Forward Pass](forward-pass.md)**
   - Input layer processing
   - Matrix multiplication and bias
   - Activation functions (ReLU vs Sigmoid)
   - Output layer computation
   - Complete data flow with tensor shapes

3. **[Loss Computation](loss-computation.md)**
   - Softmax activation
   - Cross-entropy loss
   - Why cross-entropy for classification
   - Numerical stability techniques

4. **[Backpropagation](backpropagation.md)**
   - Chain rule fundamentals
   - Gradient flow diagrams
   - ReLU backward pass
   - Matrix transpose in backprop
   - Manual vs automatic differentiation

5. **[Weight Updates](weight-updates.md)**
   - Stochastic Gradient Descent (SGD)
   - Learning rate selection
   - Mini-batch processing
   - Momentum and advanced optimizers

6. **[Complete Training Loop](complete-training-loop.md)**
   - Epoch structure
   - Data shuffling
   - Mini-batch iteration
   - Forward → Loss → Backward → Update cycle
   - Evaluation and progress tracking

### Reference Materials
- **[Code Examples](#reference-code)** - Simple and production implementations
- **[Related Documentation](#related-documentation)** - Broader learning resources

## Quick Navigation

| Document | Topics Covered | Reading Time |
|----------|----------------|--------------|
| **[Overview](mlp-walkthrough.md)** | Architecture, training pipeline, code references | 10 min |
| **[Forward Pass](forward-pass.md)** | Input→Hidden→Output, ReLU, tensor shapes | 20 min |
| **[Loss Computation](loss-computation.md)** | Softmax, cross-entropy, numerical stability | 15 min |
| **[Backpropagation](backpropagation.md)** | Chain rule, gradient flow, computational graphs | 25 min |
| **[Weight Updates](weight-updates.md)** | SGD, learning rate, mini-batches, momentum | 15 min |
| **[Training Loop](complete-training-loop.md)** | Epochs, shuffling, evaluation, full workflow | 30 min |
| **Total** | **Complete MLP understanding** | **~2 hours** |

## Learning Paths

### Path 1: Complete Beginner
**Goal:** Understand neural networks from scratch

1. Read [MLP Walkthrough Overview](mlp-walkthrough.md) (10 min)
2. Read [Forward Pass](forward-pass.md) (20 min)
3. Run `swift mlp_simple.swift` to see it in action
4. Read [Loss Computation](loss-computation.md) (15 min)
5. Read [Backpropagation](backpropagation.md) (25 min)
6. Read [Weight Updates](weight-updates.md) (15 min)
7. Read [Complete Training Loop](complete-training-loop.md) (30 min)
8. Run `swift run MNISTMLX --model mlp --epochs 10`
9. Review [`Sources/MNISTMLX/MLPModel.swift`](../Sources/MNISTMLX/MLPModel.swift)

**Estimated time:** 3-4 hours for complete understanding

### Path 2: Refresher / Reference
**Goal:** Understand specific concepts or debug issues

- **Debugging forward pass issues?** → [Forward Pass](forward-pass.md)
- **Loss not decreasing?** → [Loss Computation](loss-computation.md)
- **Vanishing/exploding gradients?** → [Backpropagation](backpropagation.md)
- **Training unstable?** → [Weight Updates](weight-updates.md)
- **Understanding training workflow?** → [Complete Training Loop](complete-training-loop.md)

### Path 3: Educator / Curriculum Developer
**Goal:** Use this material for teaching

1. Review [MLP Walkthrough Overview](mlp-walkthrough.md) for structure
2. Skim all sections to understand content depth
3. Use diagrams and code snippets in presentations
4. Reference [`mlp_simple.swift`](../mlp_simple.swift) for live coding demos
5. Point students to specific sections based on their questions

---

## Reference Code

This documentation references two main implementations:

### 1. **Simple Educational Example** ([`mlp_simple.swift`](../mlp_simple.swift))
- **218 lines** of pure Swift
- Learns XOR function (2→4→1 architecture)
- Manual backpropagation with clear variable names
- **Best for:** Understanding the basics

**Run it:**
```bash
swift mlp_simple.swift
```

### 2. **Production MLP** ([`Sources/MNISTMLX/MLPModel.swift`](../Sources/MNISTMLX/MLPModel.swift))
- **~80 lines** using automatic differentiation
- MNIST digit classification (784→512→10)
- Modern MLX framework with GPU acceleration
- **Best for:** Understanding real-world implementations

**Run it:**
```bash
swift run MNISTMLX --model mlp --epochs 10
```

---

## Concepts Covered

By the end of this walkthrough, you'll understand:

| Concept | What You'll Learn |
|---------|-------------------|
| **Neural Network Architecture** | How layers connect, what "784→512→10" means |
| **Matrix Multiplication** | Why GEMM is the core operation, tensor shape transformations |
| **Activation Functions** | ReLU, Sigmoid, why non-linearity is essential |
| **Softmax** | Converting logits to probability distributions |
| **Cross-Entropy Loss** | Measuring classification performance |
| **Backpropagation** | Chain rule, gradient flow, computational graphs |
| **Optimization** | SGD, learning rate, mini-batches |
| **Training Loop** | Epochs, batches, forward/backward passes |
| **Automatic Differentiation** | Why modern frameworks are powerful |

---

## Diagrams and Visualizations

Each section includes:
- **Architecture diagrams:** Visualizing layer structure
- **Data flow diagrams:** Showing tensor shapes at each step
- **Gradient flow diagrams:** Illustrating backpropagation
- **Numerical examples:** Concrete matrices and calculations

**ASCII art is used** for diagrams to keep everything in plain text. More sophisticated visualizations may be added in the future.

---

## Related Documentation

### Broader Learning Resources

This walkthrough is part of a larger educational ecosystem:

#### [Complete Learning Guide](../LEARNING_GUIDE.md)
**Comprehensive repository navigation** covering all implementations:
- **Beginner Path:** Start with simple examples
- **Manual Backprop Path:** Learn gradient computation from scratch
- **Modern MLX Path:** Build production models with automatic differentiation
- **GPU Optimization Path:** Understand Metal Performance Shaders
- **Comparative Study:** Manual vs auto-diff side-by-side

**Use this if:** You want to explore beyond just MLPs (CNNs, Attention, ResNets)

#### [Educational Examples](../educational/README.md)
**Standalone educational implementations** for quick reference:
- `mlp_simple.swift` - XOR problem (218 lines, pure Swift)
- `mnist_mlp.swift` - Full MNIST with manual backprop (2053 lines)
- `mnist_cnn.swift` - Convolutional neural network
- `mnist_attention_pool.swift` - Self-attention mechanism

**Use this if:** You want to see complete, runnable examples without dependencies

#### [Main Project README](../README.md)
**Project overview and quickstart:**
- Installation instructions
- Available models and benchmarks
- Contributing guidelines
- Architecture decisions

**Use this if:** You're new to the repository or want to run the production code

### How These Resources Fit Together

```
┌─────────────────────────────────────────────────────────────┐
│                   SWIFT NEURAL NETWORKS                      │
│                  Learning Ecosystem Map                      │
└─────────────────────────────────────────────────────────────┘

Main README.md ───────────┐
  │                        │
  │                        ▼
  │                  LEARNING_GUIDE.md ◄──────────┐
  │                        │                      │
  │                        ├─── Beginner Path     │
  │                        ├─── Manual Backprop   │
  │                        ├─── Modern MLX        │
  │                        └─── GPU Optimization  │
  │                                               │
  ├──── educational/                             │
  │       │                                       │
  │       ├─ README.md ──────────────────────────┘
  │       ├─ mlp_simple.swift
  │       ├─ mnist_mlp.swift
  │       ├─ mnist_cnn.swift
  │       └─ mnist_attention_pool.swift
  │
  └──── docs/ (THIS WALKTHROUGH) ◄──── YOU ARE HERE
          │
          ├─ README.md (overview)
          ├─ mlp-walkthrough.md (architecture)
          ├─ forward-pass.md (detailed)
          ├─ loss-computation.md (detailed)
          ├─ backpropagation.md (detailed)
          ├─ weight-updates.md (detailed)
          └─ complete-training-loop.md (detailed)

Sources/MNISTMLX/ ──── Production code (MLX framework)
  ├─ MLPModel.swift
  ├─ CNNModel.swift
  ├─ AttentionModel.swift
  └─ ResNetModel.swift

Sources/MNISTClassic/ ──── Educational refactor (manual backprop)
  ├─ CPUBackend.swift
  ├─ GPUBackend.swift
  └─ MPSGraphTraining.swift
```

### When to Use Each Resource

| If you want to... | Use this resource |
|-------------------|-------------------|
| **Understand MLP fundamentals** | This walkthrough (docs/) |
| **Learn how backprop actually works** | [educational/README.md](../educational/README.md) |
| **Build production ML models** | [LEARNING_GUIDE.md](../LEARNING_GUIDE.md) → Modern MLX Path |
| **Implement custom architectures** | Study Sources/MNISTMLX/ + backprop docs |
| **Optimize GPU performance** | [LEARNING_GUIDE.md](../LEARNING_GUIDE.md) → GPU Path |
| **Quick code reference** | [educational/README.md](../educational/README.md) |
| **Compare approaches** | [LEARNING_GUIDE.md](../LEARNING_GUIDE.md) → Comparative Study |

### Advanced Topics (Future Documentation)

- **CNN Walkthrough:** How convolutional networks work
- **Attention Mechanisms:** Self-attention and transformers explained
- **GPU Optimization:** Metal Performance Shaders deep dive
- **Custom Architectures:** Designing new layer types
- **Hyperparameter Tuning:** Systematic optimization strategies

---

## Prerequisites

**Math:**
- Basic arithmetic (addition, multiplication)
- Matrix multiplication (we explain this!)
- Derivatives (we give intuition, not rigorous proofs)

**Programming:**
- Basic Swift or any programming language
- Understanding of arrays/vectors
- Familiarity with loops and functions

**ML Knowledge:**
- **None required!** We explain everything from scratch.

---

## Conventions Used

Throughout this documentation:

- **Bold** for important concepts
- `Code snippets` for Swift code
- "Quoted" for specific terminology
- → for data flow (e.g., [784]→[512]→[10])
- Mathematical formulas in standard notation (e.g., y = Wx + b)

**Code blocks** show actual Swift code from our implementations with line-by-line annotations.

**Diagrams** use ASCII art for network structures and data flow.

---

## Contributing

Found an error or have a suggestion? This documentation is part of the Swift Neural Networks repository. See the main README for contribution guidelines.

### Documentation Improvements We Welcome

- **Typo fixes and clarity improvements**
- **Additional diagrams or visualizations**
- **Code examples for different use cases**
- **Translations to other languages**
- **Questions that should be addressed**

Submit issues or pull requests to the main repository.

---

## Navigation

### Quick Links
- **[Start the Walkthrough](mlp-walkthrough.md)**
- **[Complete Learning Guide](../LEARNING_GUIDE.md)**
- **[Educational Examples](../educational/README.md)**
- **[Project Home](../README.md)**

### All Walkthrough Sections
1. [MLP Walkthrough Overview](mlp-walkthrough.md) - Start here!
2. [Forward Pass](forward-pass.md) - Data flow through layers
3. [Loss Computation](loss-computation.md) - Measuring errors
4. [Backpropagation](backpropagation.md) - Computing gradients
5. [Weight Updates](weight-updates.md) - Stochastic gradient descent
6. [Complete Training Loop](complete-training-loop.md) - Full workflow

---

**Start reading:** [MLP Walkthrough Overview](mlp-walkthrough.md)
