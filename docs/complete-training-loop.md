# Complete Training Loop Walkthrough: MLP for MNIST

This document provides a comprehensive, step-by-step walkthrough of the complete training process for our Multi-Layer Perceptron (MLP) network. We'll cover everything from data preparation through multiple epochs of training, including evaluation.

## What is a Training Loop?

The **training loop** is the heart of machine learning. It's the iterative process that adjusts the network's parameters (weights and biases) to minimize prediction errors.

**Core concept:**
1. Show the network examples (forward pass)
2. Measure how wrong it is (loss computation)
3. Calculate how to improve (backward pass / gradients)
4. Make small improvements (weight updates)
5. Repeat thousands of times until it learns

---

## Complete Training Loop Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPLETE TRAINING LOOP                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INITIALIZATION (Once)
  â†“
  â€¢ Create model with random weights
  â€¢ Create optimizer (SGD with learning rate)
  â€¢ Load training and test datasets

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EPOCH LOOP (Outer Loop) - Repeat for N epochs                   â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 1. DATA SHUFFLING                                        â”‚   â”‚
â”‚  â”‚    â€¢ Shuffle training data indices                       â”‚   â”‚
â”‚  â”‚    â€¢ Prevents learning order-dependent patterns          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 2. MINI-BATCH LOOP (Inner Loop)                          â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚    FOR each batch in shuffled dataset:                  â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚    â”‚ a) Get Batch                                    â”‚  â”‚   â”‚
â”‚  â”‚    â”‚    â€¢ Extract batch_size samples                 â”‚  â”‚   â”‚
â”‚  â”‚    â”‚    â€¢ Images: [batch_size, 784]                  â”‚  â”‚   â”‚
â”‚  â”‚    â”‚    â€¢ Labels: [batch_size]                       â”‚  â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚               â†“                                         â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚    â”‚ b) Forward Pass                                 â”‚  â”‚   â”‚
â”‚  â”‚    â”‚    â€¢ Input â†’ Hidden â†’ ReLU â†’ Output             â”‚  â”‚   â”‚
â”‚  â”‚    â”‚    â€¢ Produces predictions (logits)              â”‚  â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚               â†“                                         â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚    â”‚ c) Loss Computation                             â”‚  â”‚   â”‚
â”‚  â”‚    â”‚    â€¢ Compare predictions to true labels         â”‚  â”‚   â”‚
â”‚  â”‚    â”‚    â€¢ Cross-entropy loss (single number)         â”‚  â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚               â†“                                         â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚    â”‚ d) Backward Pass                                â”‚  â”‚   â”‚
â”‚  â”‚    â”‚    â€¢ Compute gradients (âˆ‚Loss/âˆ‚W, âˆ‚Loss/âˆ‚b)    â”‚  â”‚   â”‚
â”‚  â”‚    â”‚    â€¢ Automatic differentiation (chain rule)     â”‚  â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚               â†“                                         â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚    â”‚ e) Weight Update                                â”‚  â”‚   â”‚
â”‚  â”‚    â”‚    â€¢ W = W - learning_rate Ã— gradient           â”‚  â”‚   â”‚
â”‚  â”‚    â”‚    â€¢ Optimizer updates all parameters           â”‚  â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚               â†“                                         â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚    â”‚ f) Progress Tracking                            â”‚  â”‚   â”‚
â”‚  â”‚    â”‚    â€¢ Accumulate batch loss                      â”‚  â”‚   â”‚
â”‚  â”‚    â”‚    â€¢ Update progress bar                        â”‚  â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 3. EVALUATION PHASE                                      â”‚   â”‚
â”‚  â”‚    â€¢ Test on validation/test set                         â”‚   â”‚
â”‚  â”‚    â€¢ NO weight updates (inference only)                  â”‚   â”‚
â”‚  â”‚    â€¢ Compute accuracy                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 4. EPOCH SUMMARY                                         â”‚   â”‚
â”‚  â”‚    â€¢ Print epoch number, avg loss, accuracy              â”‚   â”‚
â”‚  â”‚    â€¢ Check if we should stop (early stopping)            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
  Repeat for next epoch until:
  â€¢ Reached max epochs
  â€¢ Loss stops improving
  â€¢ Accuracy satisfactory
```

---

## 1. Initialization Phase

Before training begins, we need to set up all the components:

### Step 1.1: Create Model

```swift
// Create MLP with random initial weights
let model = MLPModel()

// Architecture:
//   Input: 784 (28Ã—28 pixels)
//   Hidden: 512 neurons
//   Output: 10 classes (digits 0-9)
//
// Total parameters: ~407,000
```

**Why random initialization?**
- All zeros would make all neurons learn the same thing
- Random breaks symmetry, allowing diverse feature learning
- MLX uses Xavier/Glorot initialization for stable gradients

### Step 1.2: Create Optimizer

```swift
// Stochastic Gradient Descent with learning rate
let learningRate: Float = 0.01
let optimizer = SGD(learningRate: learningRate)
```

**What is the learning rate?**
- Controls how big each weight update step is
- Too large: unstable, overshoots minimum
- Too small: slow convergence, gets stuck
- 0.01 is a good starting point for MNIST

### Step 1.3: Load Dataset

```swift
// Load MNIST data
let mnistData = loadMNIST()

// Training data
let trainImages = mnistData.trainImages  // [60000, 784]
let trainLabels = mnistData.trainLabels  // [60000]

// Test data
let testImages = mnistData.testImages    // [10000, 784]
let testLabels = mnistData.testLabels    // [10000]
```

### Step 1.4: Set Hyperparameters

```swift
let epochs = 10          // Number of complete passes through dataset
let batchSize = 128      // Samples per mini-batch

// Derived values
let numSamples = 60_000  // MNIST training set size
let batchesPerEpoch = (numSamples + batchSize - 1) / batchSize  // ~469 batches
```

**Hyperparameters explained:**
- **Epochs**: How many times to see the entire dataset
- **Batch size**: Samples processed before updating weights
  - Larger: more stable gradients, more memory
  - Smaller: noisier gradients, faster iterations

---

## 2. Outer Loop: Epochs

An **epoch** is one complete pass through the entire training dataset.

### Why Multiple Epochs?

One pass isn't enough to learn complex patterns. The network needs to see each example multiple times:
- Epoch 1: Learn basic patterns (edges, shapes)
- Epoch 2-5: Learn combinations (curves forming digits)
- Epoch 6-10: Fine-tune and generalize

### Epoch Loop Structure

```swift
// =============================================================================
// TRAINING LOOP: Multiple Epochs
// =============================================================================

for epoch in 1...epochs {
    print("Epoch \(epoch)/\(epochs)")

    // -------------------------------------------------------------------------
    // TRAINING PHASE: Update weights on training data
    // -------------------------------------------------------------------------
    let avgLoss = trainMLPEpoch(
        model: model,
        optimizer: optimizer,
        trainImages: trainImages,
        trainLabels: trainLabels,
        batchSize: batchSize
    )

    // -------------------------------------------------------------------------
    // EVALUATION PHASE: Test without updating weights
    // -------------------------------------------------------------------------
    let accuracy = evaluateModel(
        model: model,
        testImages: testImages,
        testLabels: testLabels
    )

    // -------------------------------------------------------------------------
    // EPOCH SUMMARY
    // -------------------------------------------------------------------------
    print("Epoch \(epoch) - Loss: \(avgLoss), Accuracy: \(accuracy * 100)%")
}
```

---

## 3. Data Shuffling

Before processing mini-batches each epoch, we **shuffle** the training data:

```swift
// Create indices for all training samples
var indices = Array(0..<numSamples)  // [0, 1, 2, ..., 59999]

// Randomly shuffle
indices.shuffle()  // [42357, 891, 13452, ..., 5623]
```

### Why Shuffle?

**Without shuffling:**
```
Batch 1: [0, 0, 0, 0, ...]  â† All zeros
Batch 2: [0, 0, 0, 0, ...]  â† Still all zeros
Batch 3: [1, 1, 1, 1, ...]  â† Now all ones
```
The model would:
- Learn zeros, forget it when seeing ones
- Learn ones, forget it when seeing twos
- Never converge properly

**With shuffling:**
```
Batch 1: [7, 2, 1, 0, 9, 4, ...]  â† Mixed digits
Batch 2: [3, 5, 1, 7, 2, 8, ...]  â† Different mix
Batch 3: [0, 4, 6, 9, 1, 3, ...]  â† Another mix
```
The model sees diverse examples in each batch, leading to:
- Better generalization
- Faster convergence
- Escapes local minima

**Visual representation:**
```
Before Shuffling (sorted by class):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0000000...111111...222222...333333...999999    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“ shuffle()
After Shuffling (random order):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7,2,1,0,9,4,3,5,1,7,2,8,0,4,6,9,1,3,5,8,2...  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Inner Loop: Mini-Batch Iteration

The **mini-batch loop** iterates through the shuffled dataset in chunks.

### Complete Mini-Batch Loop

```swift
// =============================================================================
// MINI-BATCH TRAINING LOOP
// =============================================================================

func trainMLPEpoch(
    model: MLPModel,
    optimizer: SGD,
    trainImages: MLXArray,
    trainLabels: MLXArray,
    batchSize: Int
) -> Float {
    let numSamples = trainImages.shape[0]  // 60,000
    var totalLoss: Float = 0
    var batchCount = 0

    // -------------------------------------------------------------------------
    // Step 1: SHUFFLE DATA
    // -------------------------------------------------------------------------
    var indices = Array(0..<numSamples)
    indices.shuffle()

    // -------------------------------------------------------------------------
    // Step 2: Setup Automatic Differentiation
    // -------------------------------------------------------------------------
    // This function computes both loss value AND gradients automatically!
    let lossAndGrad = valueAndGrad(model: model, mlpLoss)

    // -------------------------------------------------------------------------
    // Step 3: Progress Tracking
    // -------------------------------------------------------------------------
    let totalBatches = (numSamples + batchSize - 1) / batchSize
    let progressBar = ProgressBar(totalBatches: totalBatches)
    progressBar.start()

    // -------------------------------------------------------------------------
    // Step 4: ITERATE THROUGH MINI-BATCHES
    // -------------------------------------------------------------------------
    var start = 0
    while start < numSamples {
        // ---------------------------------------------------------------------
        // 4a. Extract Batch Indices
        // ---------------------------------------------------------------------
        let end = min(start + batchSize, numSamples)
        let batchIndices = Array(indices[start..<end]).map { Int32($0) }
        let idxArray = MLXArray(batchIndices)

        // ---------------------------------------------------------------------
        // 4b. Get Batch Data
        // ---------------------------------------------------------------------
        // Use indices to extract batch from shuffled data
        let batchImages = trainImages[idxArray]  // [batch_size, 784]
        let batchLabels = trainLabels[idxArray]  // [batch_size]

        // =====================================================================
        // 4c. THE TRAINING STEP: Forward â†’ Loss â†’ Backward â†’ Update
        // =====================================================================

        // FORWARD PASS + LOSS + BACKWARD PASS (all automatic!)
        // This single line:
        //   1. Runs forward pass: images â†’ hidden â†’ ReLU â†’ output â†’ logits
        //   2. Computes loss: cross_entropy(logits, labels)
        //   3. Backward pass: computes all gradients via chain rule
        let (loss, grads) = lossAndGrad(model, batchImages, batchLabels)

        // WEIGHT UPDATE: w = w - learning_rate Ã— gradient
        optimizer.update(model: model, gradients: grads)

        // Force evaluation (MLX uses lazy evaluation)
        eval(model, optimizer)

        // ---------------------------------------------------------------------
        // 4d. Track Progress
        // ---------------------------------------------------------------------
        let lossValue = loss.item(Float.self)
        totalLoss += lossValue
        batchCount += 1

        // Update progress bar
        progressBar.update(batch: batchCount, loss: lossValue)

        // Move to next batch
        start = end
    }

    // -------------------------------------------------------------------------
    // Step 5: Finish Epoch
    // -------------------------------------------------------------------------
    progressBar.finish()

    // Return average loss across all batches
    return totalLoss / Float(batchCount)
}
```

### Mini-Batch Processing Timeline

```
Epoch 1 (60,000 samples, batch_size=128):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Batch 1:  [0-127]     â†’ Forward â†’ Loss â†’ Backward â†’ Update
Batch 2:  [128-255]   â†’ Forward â†’ Loss â†’ Backward â†’ Update
Batch 3:  [256-383]   â†’ Forward â†’ Loss â†’ Backward â†’ Update
...
Batch 468: [59904-60031] â†’ Forward â†’ Loss â†’ Backward â†’ Update
Batch 469: [60032-59999] â†’ Forward â†’ Loss â†’ Backward â†’ Update (last batch, 96 samples)

Total: 469 weight updates per epoch
```

---

## 5. The Training Cycle: Forward â†’ Loss â†’ Backward â†’ Update

This is the **core of machine learning** - the four-step process that makes learning happen.

### Detailed Breakdown

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         THE TRAINING CYCLE (Per Mini-Batch)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT: Batch of images [128, 784] and labels [128]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: FORWARD PASS                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Images [128, 784]                                           â”‚
â”‚       â†“                                                      â”‚
â”‚  Hidden Layer: z1 = images @ W1 + b1                         â”‚
â”‚       â†“                                                      â”‚
â”‚  Activation: a1 = ReLU(z1)                                   â”‚
â”‚       â†“                                                      â”‚
â”‚  Output Layer: logits = a1 @ W2 + b2                         â”‚
â”‚       â†“                                                      â”‚
â”‚  Logits [128, 10]                                            â”‚
â”‚                                                              â”‚
â”‚  Example for one sample:                                     â”‚
â”‚    True label: 7                                             â”‚
â”‚    Predicted logits: [-1.2, 0.3, 4.8, -0.5, 1.1, ...]       â”‚
â”‚                                   â†‘                          â”‚
â”‚                            Wrongly predicts "2"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: LOSS COMPUTATION                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Cross-Entropy Loss:                                         â”‚
â”‚                                                              â”‚
â”‚    loss = -log(softmax(logits)[true_class])                 â”‚
â”‚                                                              â”‚
â”‚  For each sample:                                            â”‚
â”‚    1. Convert logits to probabilities (softmax)              â”‚
â”‚    2. Look at probability for true class                     â”‚
â”‚    3. Take negative log                                      â”‚
â”‚                                                              â”‚
â”‚  Example:                                                    â”‚
â”‚    Logits for "7": [-1.2, 0.3, 4.8, ..., 2.1, ...]          â”‚
â”‚                                           â†‘                  â”‚
â”‚                                      class 7: 2.1            â”‚
â”‚                                                              â”‚
â”‚    After softmax: [0.01, 0.03, 0.65, ..., 0.18, ...]        â”‚
â”‚                                            â†‘                 â”‚
â”‚                                   P(class=7) = 0.18          â”‚
â”‚                                                              â”‚
â”‚    Loss = -log(0.18) = 1.71  (high because confidence low)  â”‚
â”‚                                                              â”‚
â”‚  Batch average: loss = mean(all 128 sample losses)          â”‚
â”‚                      = scalar value (e.g., 0.85)            â”‚
â”‚                                                              â”‚
â”‚  This single number measures "how wrong" the model is!       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: BACKWARD PASS (Backpropagation)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Compute gradients: âˆ‚Loss/âˆ‚W1, âˆ‚Loss/âˆ‚b1, âˆ‚Loss/âˆ‚W2, ...   â”‚
â”‚                                                              â”‚
â”‚  Chain rule (automatic differentiation):                    â”‚
â”‚    Loss â† Softmax â† Output â† ReLU â† Hidden â† Input          â”‚
â”‚      â†“       â†“        â†“       â†“       â†“                     â”‚
â”‚    Gradients flow backward through each operation           â”‚
â”‚                                                              â”‚
â”‚  Gradient tells us:                                          â”‚
â”‚    â€¢ Which direction to change each weight                   â”‚
â”‚    â€¢ How much it affects the loss                            â”‚
â”‚                                                              â”‚
â”‚  Example gradient for one weight:                            â”‚
â”‚    âˆ‚Loss/âˆ‚W1[234,67] = -0.042                               â”‚
â”‚                         â†‘                                    â”‚
â”‚                   Negative means: increase this weight       â”‚
â”‚                   to reduce loss                             â”‚
â”‚                                                              â”‚
â”‚  Output: Gradient tensors matching parameter shapes         â”‚
â”‚    grads.W1: [784, 512]  (same shape as W1)                 â”‚
â”‚    grads.b1: [512]       (same shape as b1)                 â”‚
â”‚    grads.W2: [512, 10]   (same shape as W2)                 â”‚
â”‚    grads.b2: [10]        (same shape as b2)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: WEIGHT UPDATE (Gradient Descent)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  For each parameter:                                         â”‚
â”‚    new_weight = old_weight - learning_rate Ã— gradient        â”‚
â”‚                                                              â”‚
â”‚  Example (learning_rate = 0.01):                             â”‚
â”‚                                                              â”‚
â”‚    W1[234,67] = 0.523 - 0.01 Ã— (-0.042)                     â”‚
â”‚               = 0.523 + 0.00042                              â”‚
â”‚               = 0.52342                                      â”‚
â”‚                                                              â”‚
â”‚    The weight increased slightly (gradient was negative)     â”‚
â”‚                                                              â”‚
â”‚  This happens for ALL ~407,000 parameters simultaneously!    â”‚
â”‚                                                              â”‚
â”‚  After update:                                               â”‚
â”‚    â€¢ Weights have changed slightly                           â”‚
â”‚    â€¢ Model should perform slightly better                    â”‚
â”‚    â€¢ Repeat for next batch to keep improving                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Code for Training Cycle

```swift
// This is what happens inside the mini-batch loop:

// STEP 1-3: Forward pass + Loss + Backward pass (automatic!)
let (loss, grads) = lossAndGrad(model, batchImages, batchLabels)
// Returns:
//   loss: Scalar MLXArray (e.g., 0.85)
//   grads: Dictionary of gradients for each parameter

// STEP 4: Weight update
optimizer.update(model: model, gradients: grads)
// Updates all weights: w = w - lr Ã— grad

// Force evaluation (MLX is lazy)
eval(model, optimizer)
```

**What `valueAndGrad` does:**
```swift
// Under the hood (conceptual):
func valueAndGrad(model, lossFunc) {
    return { model, images, labels in
        // 1. FORWARD PASS (with gradient tracking)
        let logits = model(images)

        // 2. LOSS COMPUTATION
        let loss = crossEntropy(logits, labels)

        // 3. BACKWARD PASS (automatic chain rule)
        let grads = computeGradients(loss, model.parameters)

        return (loss, grads)
    }
}
```

---

## 6. Progress Tracking

During training, we track:
1. **Per-batch loss**: How well we're doing on current batch
2. **Average epoch loss**: Overall performance this epoch
3. **Progress bar**: Visual feedback

### Progress Bar Example

```
Epoch 1/10
[=====================================>            ] 75% (351/469) Loss: 0.234
```

### Implementation

```swift
class ProgressBar {
    let totalBatches: Int
    var currentBatch: Int = 0

    func update(batch: Int, loss: Float) {
        currentBatch = batch
        let percent = Int((Float(batch) / Float(totalBatches)) * 100)

        // Print progress
        print("\r[\(progressString(percent))] \(percent)% (\(batch)/\(totalBatches)) Loss: \(String(format: "%.3f", loss))", terminator: "")
    }

    func finish() {
        print("")  // New line after progress bar
    }
}
```

### Tracking Over Time

```
Epoch 1: Batch 100, Loss: 0.856
Epoch 1: Batch 200, Loss: 0.432
Epoch 1: Batch 300, Loss: 0.287
Epoch 1: Batch 400, Loss: 0.198
Epoch 1: Average Loss: 0.321

Epoch 2: Batch 100, Loss: 0.156
Epoch 2: Batch 200, Loss: 0.134
...

Notice: Loss decreases both within epoch and across epochs!
```

---

## 7. Evaluation Phase

After each training epoch, we evaluate on the **test set** to see how well the model generalizes.

### Key Differences: Training vs Evaluation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      TRAINING           â”‚      EVALUATION         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Use training data       â”‚ Use test data           â”‚
â”‚ Update weights          â”‚ NO weight updates       â”‚
â”‚ Compute gradients       â”‚ No gradients needed     â”‚
â”‚ Track loss              â”‚ Track accuracy          â”‚
â”‚ Mini-batches            â”‚ Can use full dataset    â”‚
â”‚ Data shuffled           â”‚ Order doesn't matter    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Evaluation Function

```swift
func evaluateModel(
    model: MLPModel,
    testImages: MLXArray,
    testLabels: MLXArray
) -> Float {
    // -------------------------------------------------------------------------
    // Forward pass ONLY (no gradient computation needed)
    // -------------------------------------------------------------------------
    let logits = model(testImages)  // [10000, 10]

    // -------------------------------------------------------------------------
    // Get predictions
    // -------------------------------------------------------------------------
    // Find class with highest logit value
    let predictions = argMax(logits, axis: 1)  // [10000]

    // -------------------------------------------------------------------------
    // Compute accuracy
    // -------------------------------------------------------------------------
    // Count how many predictions match true labels
    let correct = predictions .== testLabels  // [10000] of booleans
    let accuracy = mean(correct).item(Float.self)  // Single value: 0.0 to 1.0

    return accuracy
}
```

### Evaluation Example

```swift
// After epoch 1
let accuracy = evaluateModel(model, testImages, testLabels)
// accuracy = 0.9234 (92.34% correct)

print("Test Accuracy: \(accuracy * 100)%")
// Output: Test Accuracy: 92.34%
```

### What Accuracy Means

```
Test set: 10,000 images

Predictions:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Image   â”‚  True   â”‚ Predicted  â”‚ Result â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   img1   â”‚    7    â”‚     7      â”‚   âœ“    â”‚
â”‚   img2   â”‚    2    â”‚     2      â”‚   âœ“    â”‚
â”‚   img3   â”‚    1    â”‚     7      â”‚   âœ—    â”‚
â”‚   img4   â”‚    0    â”‚     0      â”‚   âœ“    â”‚
â”‚   ...    â”‚   ...   â”‚    ...     â”‚  ...   â”‚
â”‚  img10000â”‚    4    â”‚     4      â”‚   âœ“    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Correct: 9,234 out of 10,000
Accuracy: 9,234 / 10,000 = 0.9234 = 92.34%
```

---

## 8. Complete Training Example with Full Annotations

Here's a complete, runnable training loop with line-by-line explanations:

```swift
// =============================================================================
// COMPLETE TRAINING LOOP EXAMPLE
// =============================================================================

import Foundation
import MLX
import MLXNN
import MLXOptimizers

func trainMNIST() {
    // =========================================================================
    // INITIALIZATION
    // =========================================================================

    // Step 1: Create model with random weights
    let model = MLPModel()
    print("Model created with \(model.parameters().count) parameter tensors")

    // Step 2: Create optimizer
    let learningRate: Float = 0.01
    let optimizer = SGD(learningRate: learningRate)
    print("Optimizer: SGD with learning rate \(learningRate)")

    // Step 3: Load MNIST dataset
    print("Loading MNIST dataset...")
    let mnist = loadMNIST()
    let trainImages = mnist.trainImages  // [60000, 784]
    let trainLabels = mnist.trainLabels  // [60000]
    let testImages = mnist.testImages    // [10000, 784]
    let testLabels = mnist.testLabels    // [10000]
    print("Dataset loaded: 60,000 training samples, 10,000 test samples")

    // Step 4: Set hyperparameters
    let epochs = 10
    let batchSize = 128
    print("Training for \(epochs) epochs with batch size \(batchSize)")
    print("")

    // =========================================================================
    // TRAINING LOOP: EPOCHS
    // =========================================================================

    for epoch in 1...epochs {
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("Epoch \(epoch)/\(epochs)")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

        // =====================================================================
        // TRAINING PHASE: One complete pass through training data
        // =====================================================================

        let epochStartTime = Date()

        // Train for one epoch (see detailed function below)
        let avgLoss = trainMLPEpoch(
            model: model,
            optimizer: optimizer,
            trainImages: trainImages,
            trainLabels: trainLabels,
            batchSize: batchSize
        )

        let epochDuration = Date().timeIntervalSince(epochStartTime)

        // =====================================================================
        // EVALUATION PHASE: Test on held-out data
        // =====================================================================

        print("Evaluating on test set...")
        let accuracy = evaluateModel(
            model: model,
            testImages: testImages,
            testLabels: testLabels
        )

        // =====================================================================
        // EPOCH SUMMARY
        // =====================================================================

        print("")
        print("Epoch \(epoch) Summary:")
        print("  Average Loss: \(String(format: "%.4f", avgLoss))")
        print("  Test Accuracy: \(String(format: "%.2f", accuracy * 100))%")
        print("  Duration: \(String(format: "%.1f", epochDuration))s")
        print("")

        // =====================================================================
        // EARLY STOPPING (Optional)
        // =====================================================================

        if accuracy > 0.98 {
            print("ğŸ‰ Reached 98% accuracy! Stopping early.")
            break
        }
    }

    // =========================================================================
    // TRAINING COMPLETE
    // =========================================================================

    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("Training complete!")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
}

// =============================================================================
// TRAINING EPOCH FUNCTION: Inner loop with mini-batches
// =============================================================================

func trainMLPEpoch(
    model: MLPModel,
    optimizer: SGD,
    trainImages: MLXArray,
    trainLabels: MLXArray,
    batchSize: Int
) -> Float {

    let numSamples = trainImages.shape[0]  // 60,000
    var totalLoss: Float = 0
    var batchCount = 0

    // =========================================================================
    // STEP 1: SHUFFLE DATA
    // =========================================================================
    // Create random permutation of indices
    var indices = Array(0..<numSamples)
    indices.shuffle()

    print("  Data shuffled: presenting samples in random order")

    // =========================================================================
    // STEP 2: SETUP AUTOMATIC DIFFERENTIATION
    // =========================================================================
    // Create function that computes loss AND gradients
    let lossAndGrad = valueAndGrad(model: model, mlpLoss)

    print("  Automatic differentiation enabled")

    // =========================================================================
    // STEP 3: PROGRESS BAR SETUP
    // =========================================================================
    let totalBatches = (numSamples + batchSize - 1) / batchSize
    let progressBar = ProgressBar(totalBatches: totalBatches)

    print("  Training on \(totalBatches) mini-batches:")
    progressBar.start()

    // =========================================================================
    // STEP 4: MINI-BATCH LOOP
    // =========================================================================
    var start = 0
    while start < numSamples {

        // ---------------------------------------------------------------------
        // 4a. Get batch indices
        // ---------------------------------------------------------------------
        let end = min(start + batchSize, numSamples)
        let batchIndices = Array(indices[start..<end]).map { Int32($0) }
        let idxArray = MLXArray(batchIndices)

        // ---------------------------------------------------------------------
        // 4b. Extract batch data using shuffled indices
        // ---------------------------------------------------------------------
        let batchImages = trainImages[idxArray]  // [batch_size, 784]
        let batchLabels = trainLabels[idxArray]  // [batch_size]

        // =====================================================================
        // 4c. THE TRAINING CYCLE: Forward â†’ Loss â†’ Backward â†’ Update
        // =====================================================================

        // FORWARD PASS + LOSS COMPUTATION + BACKWARD PASS
        // This computes:
        //   1. predictions = model(batchImages)
        //   2. loss = crossEntropy(predictions, batchLabels)
        //   3. gradients = backprop(loss)
        let (loss, grads) = lossAndGrad(model, batchImages, batchLabels)

        // WEIGHT UPDATE
        // For each parameter: w = w - learning_rate Ã— gradient
        optimizer.update(model: model, gradients: grads)

        // Force evaluation (MLX is lazy - doesn't compute until needed)
        eval(model, optimizer)

        // ---------------------------------------------------------------------
        // 4d. Track progress
        // ---------------------------------------------------------------------
        let lossValue = loss.item(Float.self)
        totalLoss += lossValue
        batchCount += 1

        // Update progress bar
        progressBar.update(batch: batchCount, loss: lossValue)

        // Move to next batch
        start = end
    }

    // =========================================================================
    // STEP 5: FINISH EPOCH
    // =========================================================================
    progressBar.finish()

    // Return average loss across all batches
    return totalLoss / Float(batchCount)
}

// =============================================================================
// EVALUATION FUNCTION: Test accuracy without training
// =============================================================================

func evaluateModel(
    model: MLPModel,
    testImages: MLXArray,
    testLabels: MLXArray
) -> Float {

    // Forward pass (no gradient computation needed)
    let logits = model(testImages)

    // Get predicted classes (argmax)
    let predictions = argMax(logits, axis: 1)

    // Count correct predictions
    let correct = predictions .== testLabels

    // Return accuracy (fraction correct)
    return mean(correct).item(Float.self)
}

// =============================================================================
// RUN TRAINING
// =============================================================================

trainMNIST()
```

### Expected Output

```
Model created with 4 parameter tensors
Optimizer: SGD with learning rate 0.01
Loading MNIST dataset...
Dataset loaded: 60,000 training samples, 10,000 test samples
Training for 10 epochs with batch size 128

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Epoch 1/10
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Data shuffled: presenting samples in random order
  Automatic differentiation enabled
  Training on 469 mini-batches:
[=====================================>] 100% (469/469) Loss: 0.234
Evaluating on test set...

Epoch 1 Summary:
  Average Loss: 0.3214
  Test Accuracy: 91.23%
  Duration: 12.3s

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Epoch 2/10
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Data shuffled: presenting samples in random order
  Automatic differentiation enabled
  Training on 469 mini-batches:
[=====================================>] 100% (469/469) Loss: 0.156
Evaluating on test set...

Epoch 2 Summary:
  Average Loss: 0.1876
  Test Accuracy: 94.56%
  Duration: 11.8s

...

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Epoch 8/10
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Data shuffled: presenting samples in random order
  Automatic differentiation enabled
  Training on 469 mini-batches:
[=====================================>] 100% (469/469) Loss: 0.021
Evaluating on test set...

Epoch 8 Summary:
  Average Loss: 0.0312
  Test Accuracy: 98.12%
  Duration: 11.5s

ğŸ‰ Reached 98% accuracy! Stopping early.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Training complete!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## 9. Training Progression Visualization

### How Loss Decreases

```
Loss Over Time:

1.0 â”¤
    â”‚â—
0.9 â”¤ â—
    â”‚  â—
0.8 â”¤   â—
    â”‚    â—
0.7 â”¤     â—
    â”‚      â—
0.6 â”¤       â—
    â”‚        â—â—
0.5 â”¤          â—â—
    â”‚            â—â—
0.4 â”¤              â—â—â—
    â”‚                 â—â—â—
0.3 â”¤                    â—â—â—â—
    â”‚                        â—â—â—â—â—
0.2 â”¤                             â—â—â—â—â—â—
    â”‚                                   â—â—â—â—â—â—â—
0.1 â”¤                                          â—â—â—â—â—â—
    â”‚                                                â—â—â—â—
0.0 â”¤
    â””â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬
     0    1    2    3    4    5    6    7    8    9
                         Epoch

Note: Rapid improvement early, then gradual refinement
```

### How Accuracy Increases

```
Accuracy Over Time:

100%â”¤                                          â—â—â—â—â—
    â”‚                                    â—â—â—â—â—â—
 95%â”¤                              â—â—â—â—â—â—
    â”‚                        â—â—â—â—â—â—
 90%â”¤                  â—â—â—â—â—â—
    â”‚            â—â—â—â—â—â—
 85%â”¤      â—â—â—â—â—â—
    â”‚  â—â—â—â—
 80%â”¤â—â—
    â”‚
 75%â”¤
    â”‚
 70%â”¤
    â””â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬
     0    1    2    3    4    5    6    7    8    9
                         Epoch

Typical MNIST MLP progression:
  Epoch 1:  80-85%  (learns basic patterns)
  Epoch 2:  90-92%  (refines features)
  Epoch 5:  95-96%  (good generalization)
  Epoch 10: 97-98%  (near-optimal for MLP)
```

---

## 10. Key Takeaways

### Essential Concepts

1. **Two Nested Loops**:
   - Outer: Epochs (complete passes through dataset)
   - Inner: Mini-batches (chunks of data)

2. **Data Shuffling**:
   - Shuffle once per epoch
   - Prevents order-dependent learning
   - Helps generalization

3. **The Training Cycle** (per mini-batch):
   ```
   Forward Pass â†’ Loss â†’ Backward Pass â†’ Update Weights
   ```

4. **Progress Tracking**:
   - Monitor loss (training performance)
   - Monitor accuracy (test performance)
   - Watch for overfitting

5. **Evaluation**:
   - Always test on held-out data
   - No gradient computation needed
   - Measures generalization ability

### Why This Works

**Stochastic Gradient Descent (SGD):**
- Update weights after each mini-batch (not entire dataset)
- Noisy gradients help escape local minima
- Much faster than computing on full dataset

**Multiple Epochs:**
- Network needs to see examples multiple times
- Early epochs: learn basic patterns
- Later epochs: fine-tune and generalize

**Batching:**
- Efficient GPU utilization
- Stable gradient estimates
- Memory efficient

---

## 11. Common Questions

### Q: Why use mini-batches instead of full dataset?

**Full dataset (Batch Gradient Descent):**
- Pros: Stable, smooth convergence
- Cons: Slow, requires lots of memory, can get stuck

**Single sample (Online/Stochastic):**
- Pros: Fast updates, escapes local minima
- Cons: Very noisy, unstable

**Mini-batches (best of both):**
- Pros: Good gradient estimates, efficient, GPU-friendly
- Cons: One more hyperparameter to tune

### Q: How do I choose batch size?

**Common values:** 32, 64, 128, 256

**Larger batches (256+):**
- More stable gradients
- Better GPU utilization
- Requires more memory
- May generalize worse

**Smaller batches (32-64):**
- More noise â†’ helps escape local minima
- Less memory
- More updates per epoch
- May generalize better

**For MNIST:** 128 is a good balance

### Q: How many epochs should I train?

Train until:
- Loss stops decreasing
- Accuracy plateaus
- Validation accuracy starts decreasing (overfitting)

**For MNIST MLP:**
- 5-10 epochs usually sufficient
- More won't hurt but won't help much

### Q: What if loss increases?

Possible causes:
- Learning rate too high (most common)
- Bug in code
- Gradient explosion
- Bad initialization

**Solution:** Reduce learning rate by 10x

### Q: What's the difference between loss and accuracy?

**Loss:**
- Continuous measure of error
- What the network optimizes
- Can be any positive value
- Lower is better

**Accuracy:**
- Discrete measure (correct/incorrect)
- Easy to interpret
- Bounded 0-100%
- Higher is better

---

## 12. References

### Code Locations

- **Full MLP Model**: `Sources/MNISTMLX/MLPModel.swift`
- **Training Function**: `trainMLPEpoch()` in MLPModel.swift
- **Simple Example**: `mlp_simple.swift` (educational XOR example)

### Related Documentation

- **Forward Pass**: `docs/forward-pass.md`
- **Loss Computation**: `docs/loss-computation.md`
- **Backpropagation**: `docs/backpropagation.md`
- **Weight Updates**: `docs/weight-updates.md`
- **MLP Overview**: `docs/mlp-walkthrough.md`

---

## Summary

The **complete training loop** brings together all the pieces of neural network learning:

1. **Initialization**: Create model, optimizer, load data
2. **Epochs**: Multiple complete passes through dataset
3. **Shuffling**: Randomize order each epoch
4. **Mini-batches**: Process data in chunks
5. **Training Cycle**: Forward â†’ Loss â†’ Backward â†’ Update
6. **Progress Tracking**: Monitor performance
7. **Evaluation**: Test on held-out data
8. **Iteration**: Repeat until converged

This is the foundation of how neural networks learn - a simple but powerful algorithm that enables models to learn complex patterns from data!
