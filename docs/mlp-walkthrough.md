# MLP Walkthrough: Neural Networks from First Principles

## What You'll Learn

This walkthrough teaches you **exactly how a neural network works** by walking through every step of training a Multi-Layer Perceptron (MLP) on the MNIST digit dataset.

By the end, you'll understand:
- ‚úÖ What happens when an image enters the network
- ‚úÖ How matrix multiplications transform data
- ‚úÖ Why activation functions create non-linearity
- ‚úÖ How loss measures prediction quality
- ‚úÖ How gradients flow backward through layers
- ‚úÖ How weights get updated to improve performance

**No black boxes. No hand-waving. Just clear explanations with code.**

---

## The MLP Architecture

Our MNIST classifier uses a simple 3-layer architecture:

```
Input Layer    Hidden Layer    Output Layer
[784 pixels] ‚Üí [512 neurons] ‚Üí [10 classes]
```

**In detail:**

1. **Input Layer:** 784 pixels (28√ó28 grayscale image flattened)
2. **Hidden Layer:** 512 neurons with ReLU activation
3. **Output Layer:** 10 neurons (one per digit 0-9) with Softmax

### Complete Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    MULTI-LAYER PERCEPTRON (MLP)                          ‚îÇ
‚îÇ                      MNIST Digit Classification                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

INPUT IMAGE (28√ó28 pixels)
        ‚îÉ
        ‚îÉ Flatten
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Input Layer  ‚îÇ     784 features (28√ó28 pixels flattened)
‚îÇ   [N, 784]    ‚îÇ     N = batch size
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     Each pixel: normalized value 0.0-1.0
        ‚îÇ
        ‚îÇ Matrix Multiply: x @ W1 + b1
        ‚îÇ   W1: [784, 512] - 401,408 parameters
        ‚îÇ   b1: [512]      - 512 parameters
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Hidden Layer  ‚îÇ     512 neurons (pre-activation)
‚îÇ   [N, 512]    ‚îÇ     z1 = x @ W1 + b1
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     Linear transformation
        ‚îÇ
        ‚îÇ Activation: ReLU(z1) = max(0, z1)
        ‚îÇ   ‚Ä¢ Introduces non-linearity
        ‚îÇ   ‚Ä¢ Enables learning complex patterns
        ‚îÇ   ‚Ä¢ ~50% sparsity (zeros out negative values)
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Activated   ‚îÇ     512 neurons (post-activation)
‚îÇ   [N, 512]    ‚îÇ     a1 = ReLU(z1)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     All values ‚â• 0
        ‚îÇ
        ‚îÇ Matrix Multiply: a1 @ W2 + b2
        ‚îÇ   W2: [512, 10]  - 5,120 parameters
        ‚îÇ   b2: [10]       - 10 parameters
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Output Layer  ‚îÇ     10 class scores (logits)
‚îÇ   [N, 10]     ‚îÇ     logits = a1 @ W2 + b2
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     One score per digit (0-9)
        ‚îÇ
        ‚îÇ Softmax: exp(logit[i]) / Œ£exp(logit[j])
        ‚îÇ   ‚Ä¢ Converts logits to probabilities
        ‚îÇ   ‚Ä¢ All probabilities sum to 1.0
        ‚îÇ   ‚Ä¢ Applied during loss computation
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Probabilities ‚îÇ     10 probability values
‚îÇ   [N, 10]     ‚îÇ     p = softmax(logits)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     Predicted class = argmax(p)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

PARAMETER SUMMARY:
  Layer 1 (Input ‚Üí Hidden):  784 √ó 512 + 512 = 401,920 parameters
  Layer 2 (Hidden ‚Üí Output): 512 √ó 10  + 10  = 5,130 parameters
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  TOTAL:                                       407,050 parameters

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
```

### Detailed Layer Connectivity

```
        Input (784)              Hidden (512)           Output (10)
             ‚îÉ                        ‚îÉ                     ‚îÉ
             ‚îÉ                        ‚îÉ                     ‚îÉ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Pixel 0      ‚îÇ       ‚îÇ   Neuron 0     ‚îÇ    ‚îÇ   Class 0    ‚îÇ
    ‚îÇ   Pixel 1      ‚îÇ       ‚îÇ   Neuron 1     ‚îÇ    ‚îÇ   Class 1    ‚îÇ
    ‚îÇ   Pixel 2      ‚îÇ‚îÄ‚îÄW1‚îÄ‚îÄ‚Üí‚îÇ   Neuron 2     ‚îÇ‚îÄ‚îÄW2‚îÇ   Class 2    ‚îÇ
    ‚îÇ      ...       ‚îÇ  +b1  ‚îÇ      ...       ‚îÇ +b2‚îÇ     ...      ‚îÇ
    ‚îÇ   Pixel 783    ‚îÇ       ‚îÇ   Neuron 511   ‚îÇ    ‚îÇ   Class 9    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚îÇ                     ‚îÇ
                                   ReLU                 Softmax
                                     ‚Üì                     ‚Üì
                              (Non-linearity)      (Probabilities)
```

### Data Flow with Tensor Shapes

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        FORWARD PASS DATA FLOW                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Step 1: Input
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Input:  [N, 28, 28]    ‚Üê Batch of grayscale images
    ‚Üì
  Flatten
    ‚Üì
  x:      [N, 784]       ‚Üê Flattened pixel values


Step 2: Hidden Layer (Linear Transform)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  x:      [N, 784]
  W1:     [784, 512]     ‚Üê Learned weights
  b1:     [512]          ‚Üê Learned biases
    ‚Üì
  z1 = x @ W1 + b1
    ‚Üì
  z1:     [N, 512]       ‚Üê Pre-activation


Step 3: Activation (Non-linearity)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  z1:     [N, 512]
    ‚Üì
  a1 = ReLU(z1) = max(0, z1)
    ‚Üì
  a1:     [N, 512]       ‚Üê Post-activation (all ‚â• 0)


Step 4: Output Layer (Classification)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  a1:     [N, 512]
  W2:     [512, 10]      ‚Üê Learned weights
  b2:     [10]           ‚Üê Learned biases
    ‚Üì
  logits = a1 @ W2 + b2
    ‚Üì
  logits: [N, 10]        ‚Üê Class scores (unnormalized)


Step 5: Softmax (Probability Distribution)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  logits: [N, 10]
    ‚Üì
  probs = softmax(logits)
    ‚Üì
  probs:  [N, 10]        ‚Üê Probabilities (sum to 1.0)
    ‚Üì
  prediction = argmax(probs, axis=1)
    ‚Üì
  prediction: [N]        ‚Üê Predicted digit (0-9)
```

**Total parameters:** 784√ó512 + 512 + 512√ó10 + 10 = **407,050 learnable weights**

---

## The Training Pipeline

Training a neural network involves repeating this cycle:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Forward   ‚îÇ - Pass input through network to get predictions
‚îÇ    Pass      ‚îÇ   (Input ‚Üí Hidden ‚Üí Output)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Compute   ‚îÇ - Measure how wrong the predictions are
‚îÇ    Loss      ‚îÇ   (Cross-entropy between prediction & truth)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. Backward  ‚îÇ - Calculate how to adjust weights
‚îÇ    Pass      ‚îÇ   (Compute gradients via backpropagation)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Update    ‚îÇ - Adjust weights to reduce loss
‚îÇ    Weights   ‚îÇ   (Gradient descent: W = W - lr*grad)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ
          Repeat for all batches in epoch
```

**Each section below dives deep into one of these steps.**

---

## Section 1: Forward Pass

**What happens:** Input data flows through the network to produce predictions.

### Steps:

1. **Input Layer ‚Üí Hidden Layer**
   ```swift
   // Matrix multiplication + bias
   hidden = input @ W1 + b1    // Shape: [N, 784] @ [784, 512] ‚Üí [N, 512]
   ```

2. **Apply Activation**
   ```swift
   // ReLU: max(0, x)
   hidden = relu(hidden)       // Shape stays [N, 512]
   ```

3. **Hidden Layer ‚Üí Output Layer**
   ```swift
   // Matrix multiplication + bias
   logits = hidden @ W2 + b2   // Shape: [N, 512] @ [512, 10] ‚Üí [N, 10]
   ```

4. **Apply Softmax (implicitly in loss)**
   ```swift
   // Convert logits to probabilities
   probabilities = softmax(logits)  // Shape: [N, 10], each row sums to 1
   ```

**Why ReLU?** Without activation functions, the network is just nested linear transformations, which collapse to a single linear function. ReLU adds non-linearity, allowing the network to learn complex patterns.

**üìñ Read the detailed explanation:** [Forward Pass Documentation](forward-pass.md)

---

## Section 2: Loss Computation

**What happens:** We measure how wrong our predictions are.

### Cross-Entropy Loss

For classification, we use **cross-entropy loss**:

```
Loss = -log(p_correct_class)
```

Where `p_correct_class` is the predicted probability for the true class.

**Example:**
- True label: 5 (digit "5")
- Predicted probabilities: [0.01, 0.02, 0.05, 0.10, 0.15, **0.50**, 0.05, 0.04, 0.05, 0.03]
- Loss = -log(0.50) ‚âà 0.693

**Good predictions** (high probability for correct class) ‚Üí **Low loss**
**Bad predictions** (low probability for correct class) ‚Üí **High loss**

**Why cross-entropy?** It penalizes confident wrong predictions heavily and encourages the model to be accurate and confident.

**üìñ Read the detailed explanation:** [Loss Computation Documentation](loss-computation.md)

---

## Section 3: Backpropagation

**What happens:** We calculate how each weight contributed to the loss, using the chain rule.

### Gradient Flow

Gradients flow **backward** through the network:

```
Loss
  ‚Üì
‚àÇLoss/‚àÇlogits (from softmax + cross-entropy)
  ‚Üì
‚àÇLoss/‚àÇW2, ‚àÇLoss/‚àÇb2 (output layer gradients)
  ‚Üì
‚àÇLoss/‚àÇhidden (before ReLU)
  ‚Üì
‚àÇLoss/‚àÇhidden_pre_relu (apply ReLU mask)
  ‚Üì
‚àÇLoss/‚àÇW1, ‚àÇLoss/‚àÇb1 (input layer gradients)
```

### Key Insights

1. **Chain Rule:** Gradients multiply as we go backward
   ```
   ‚àÇLoss/‚àÇW1 = ‚àÇLoss/‚àÇhidden √ó ‚àÇhidden/‚àÇW1
   ```

2. **ReLU Backward:** Gradient passes through only where input was > 0
   ```swift
   grad_hidden = grad_output * (hidden > 0)  // Masking
   ```

3. **Matrix Transpose:** Appears in backprop to match dimensions
   ```swift
   ‚àÇLoss/‚àÇW2 = hidden.T @ ‚àÇLoss/‚àÇlogits
   ```

**Manual vs Automatic:** Modern frameworks (like MLX) compute gradients automatically. Understanding manual backprop helps you debug and design custom architectures.

**üìñ Read the detailed explanation:** [Backpropagation Documentation](backpropagation.md)

---

## Section 4: Weight Updates

**What happens:** We adjust weights in the direction that reduces loss.

### Stochastic Gradient Descent (SGD)

The update rule is simple:

```swift
W = W - learning_rate * gradient
```

**Parameters:**
- **Learning rate** (e.g., 0.01): How big a step to take
  - Too large ‚Üí Overshooting, unstable training
  - Too small ‚Üí Slow convergence

- **Mini-batch:** Update weights using a subset of data (e.g., 64 images)
  - Faster than full-batch (all data)
  - More stable than single example (SGD)

**Example:**
```swift
// If gradient is [0.5, -0.2, 0.3] and learning_rate is 0.01
W_new = W_old - 0.01 * [0.5, -0.2, 0.3]
      = W_old - [0.005, -0.002, 0.003]
```

**Why "Stochastic"?** We use random mini-batches instead of the full dataset, adding noise that can help escape local minima.

**üìñ Read the detailed explanation:** [Weight Updates Documentation](weight-updates.md)

---

## Section 5: Complete Training Loop

**What happens:** We tie all the pieces together into a training loop.

### Pseudocode

```swift
for epoch in 1...num_epochs {
    // Shuffle data each epoch
    shuffle(training_data)

    // Process in mini-batches
    for batch in training_data.batches(size: batch_size) {
        // 1. Forward pass
        predictions = model.forward(batch.images)

        // 2. Compute loss
        loss = crossEntropy(predictions, batch.labels)

        // 3. Backward pass (compute gradients)
        gradients = model.backward(loss)

        // 4. Update weights
        optimizer.update(model.weights, gradients)
    }

    // Evaluate on validation set
    accuracy = evaluate(model, validation_data)
    print("Epoch \(epoch): Loss = \(loss), Accuracy = \(accuracy)%")
}
```

### Key Concepts

1. **Epochs:** Complete passes through the dataset
2. **Mini-batches:** Subsets of data processed together (vectorization speedup)
3. **Shuffling:** Prevents the model from learning data order
4. **Evaluation:** Check performance on held-out data

**Typical MNIST Results:**
- Epoch 1: ~90% accuracy
- Epoch 5: ~97% accuracy
- Epoch 10: ~98% accuracy

**üìñ Read the detailed explanation:** [Complete Training Loop Documentation](complete-training-loop.md)

---

## Code References

### Where to Find the Code

This walkthrough references two implementations:

#### 1. Simple Educational Example: [`mlp_simple.swift`](../mlp_simple.swift)

**Characteristics:**
- 218 lines of pure Swift
- Learns XOR function (2‚Üí4‚Üí1 network)
- Manual backpropagation with clear variable names
- No external dependencies

**Best for:** First-time learners

**Run it:**
```bash
swift mlp_simple.swift
```

**Expected output:**
```
Training simple MLP to learn XOR...
Epoch 0, Loss: 0.6931
Epoch 1000, Loss: 0.0123
...
Final predictions close to [0, 1, 1, 0]
```

#### 2. Production MNIST MLP: [`Sources/MNISTMLX/MLPModel.swift`](../Sources/MNISTMLX/MLPModel.swift)

**Characteristics:**
- ~80 lines using automatic differentiation (MLX framework)
- MNIST digit classification (784‚Üí512‚Üí10)
- GPU-accelerated by default
- Production-quality code

**Best for:** Understanding modern ML workflows

**Run it:**
```bash
swift build
swift run MNISTMLX --model mlp --epochs 10
```

**Expected output:**
```
Epoch 1: Loss: 0.2341, Accuracy: 93.2%
Epoch 2: Loss: 0.1245, Accuracy: 96.1%
...
Epoch 10: Loss: 0.0456, Accuracy: 98.3%
```

### Code Reading Guide

**If you want to understand manual backprop:**
1. Read [`mlp_simple.swift`](../mlp_simple.swift) - Start here!
2. Read [`mnist_mlp.swift`](../mnist_mlp.swift) - Full MNIST with manual gradients
3. Compare with [`Sources/MNISTMLX/MLPModel.swift`](../Sources/MNISTMLX/MLPModel.swift) - See auto-diff

**If you want to build production models:**
1. Read [`Sources/MNISTMLX/MLPModel.swift`](../Sources/MNISTMLX/MLPModel.swift) - Modern approach
2. Skim [`mnist_mlp.swift`](../mnist_mlp.swift) - Understand what MLX is doing under the hood
3. Explore other models in `Sources/MNISTMLX/` - CNN, Attention, ResNet

---

## Next Steps

### 1. Read the Detailed Sections

Work through each section in order:
1. **[Forward Pass](forward-pass.md)** - How data flows through the network
2. **[Loss Computation](loss-computation.md)** - Measuring prediction quality
3. **[Backpropagation](backpropagation.md)** - Computing gradients via chain rule
4. **[Weight Updates](weight-updates.md)** - Stochastic gradient descent
5. **[Complete Training Loop](complete-training-loop.md)** - Tying it all together

### 2. Run the Code

Get hands-on experience:
```bash
# Simple XOR example
swift mlp_simple.swift

# Full MNIST MLP
swift run MNISTMLX --model mlp --epochs 10

# MNIST CNN (better accuracy)
swift run MNISTMLX --model cnn --epochs 5
```

### 3. Experiment

Modify the code to deepen understanding:
- Change learning rate (try 0.001, 0.01, 0.1)
- Adjust hidden layer size (256, 512, 1024)
- Add more layers
- Try different activation functions

### 4. Explore Advanced Topics

Once you understand MLPs:
- **Convolutional Neural Networks (CNNs):** Spatial pattern recognition
- **Attention Mechanisms:** Transformers and self-attention
- **Residual Networks (ResNets):** Skip connections for deep networks

See the [main README](../README.md) for all available implementations.

---

## Conceptual Roadmap

Here's what each section teaches:

| Section | Core Question Answered |
|---------|------------------------|
| **Forward Pass** | How does input data become predictions? |
| **Loss Computation** | How do we measure prediction quality? |
| **Backpropagation** | How do we calculate weight adjustments? |
| **Weight Updates** | How do weights improve over time? |
| **Training Loop** | How does everything fit together? |

**By the end, you'll know:**
- What every line of an MLP implementation does
- Why neural networks need non-linearity
- How gradients flow through computational graphs
- Why automatic differentiation is powerful
- How to debug training issues

---

## Philosophy of This Walkthrough

### Principles

1. **Intuition over formalism:** We explain *why*, not just *what*
2. **Code over math:** Executable examples, not just equations
3. **Concrete over abstract:** Real numbers, not just symbols
4. **Complete over partial:** End-to-end understanding, no gaps

### What Makes This Different

- **Line-by-line code explanations** - Every operation is annotated
- **Tensor shape tracking** - Know the dimensions at each step
- **Numerical examples** - See actual matrices and calculations
- **Gradient flow diagrams** - Visualize backpropagation
- **Manual vs auto-diff comparison** - Understand what frameworks do

**Goal:** After this walkthrough, neural networks won't feel like magic anymore.

---

## Related Documentation

- **[Main Documentation Index](README.md)** - All documentation sections
- **[Learning Guide](../LEARNING_GUIDE.md)** - Complete repository navigation
- **[Educational Implementations](../educational/README.md)** - Standalone examples

---

## Questions?

As you go through this walkthrough, keep these questions in mind:

- **Forward Pass:** What shape is this tensor? Why this matrix size?
- **Loss:** Why does low probability cause high loss?
- **Backprop:** Where does the transpose come from? Why multiply gradients?
- **Updates:** What happens if learning rate is too high/low?

The detailed sections answer all of these!

---

---

## Navigation

- **‚¨ÖÔ∏è Back:** [Documentation Home](README.md)
- **‚û°Ô∏è Next:** [Forward Pass](forward-pass.md)

### All Sections
1. **MLP Walkthrough Overview** (you are here)
2. [Forward Pass](forward-pass.md)
3. [Loss Computation](loss-computation.md)
4. [Backpropagation](backpropagation.md)
5. [Weight Updates](weight-updates.md)
6. [Complete Training Loop](complete-training-loop.md)

### Related Resources
- **[Learning Guide](../LEARNING_GUIDE.md)** - Complete repository navigation and learning paths
- **[Educational Examples](../educational/README.md)** - Standalone educational implementations
- **[Main README](../README.md)** - Project overview and quickstart

---

**Happy learning!** üß†
